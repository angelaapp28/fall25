{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelaapp28/fall25/blob/main/Copy_of_DeepLearning_Fall2025_hw4_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3G5iOckPDFC"
      },
      "source": [
        "# Deep Learning (Fall 2025) - Homework 4\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*Developed by Hongtau Wu & Suzanna Sia*\n",
        "\n",
        "This notebook contains all starter code for Homework 4. Please read the written assignment carefully to ensure you include all necessary outputs in your final report. Your final submission (a single zip file) should include this notebook (.ipynb file) and a PDF of this notebook with all cell outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-QLXT-YPDFE"
      },
      "source": [
        "## Problem 1a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgqW4Ud3PDFE"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwpCO80kPDFF"
      },
      "outputs": [],
      "source": [
        "## External Libararies\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "from matplotlib.collections import PatchCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdNgZM70PDFF"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c92eg0MRracO"
      },
      "outputs": [],
      "source": [
        "## Spectify Path to Provided Data Here\n",
        "DATA_PATH = '/content/DeepLearning_Fall2025_hw4_prob1_data (1).npy'\n",
        "\n",
        "## Load Data and Check Dimensionality\n",
        "data = np.load(DATA_PATH)\n",
        "Y = data[:,2]\n",
        "X = data[:,0:2]\n",
        "print(\"Y:\", Y.shape)\n",
        "print(\"X:\", X.shape)\n",
        "\n",
        "## Polygon Boundaries\n",
        "p = [[[500, 1000], [300, 800], [400, 600], [600, 600], [700, 800]],\n",
        "     [[500, 600], [100, 400], [300, 200], [700, 200], [900, 400]]]\n",
        "p = np.asarray(p)\n",
        "p0 = p[0]\n",
        "p1 = p[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sjLQEAF7-cMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw0Qur2lPDFG"
      },
      "source": [
        "### Visualization Code\n",
        "\n",
        "Do not touch any of the visualization code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLtNWWcIvVYk"
      },
      "outputs": [],
      "source": [
        "## Helper code for visualisation (No Need to Touch)\n",
        "def visualize_polygons(p0, p1):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    patches = []\n",
        "    polygon1 = Polygon(p0, closed=True)\n",
        "    polygon2 = Polygon(p1, closed=True)\n",
        "    patches.append(polygon1)\n",
        "    patches.append(polygon2)\n",
        "    p = PatchCollection(patches, cmap=matplotlib.cm.jet, alpha=0.4)\n",
        "    ax.add_collection(p)\n",
        "    ax.autoscale_view()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_datapoints(X, Y):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    assert(X.shape[0] == Y.shape[0])\n",
        "    fig, ax = plt.subplots()\n",
        "    npts = 60000\n",
        "    col = np.where(Y[:npts]==1,'m','b')\n",
        "    x1 = X[:npts][:,0]\n",
        "    x2 = X[:npts][:,1]\n",
        "    ax.scatter(x1, x2, s=0.5, c=col, zorder=1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yDFI7mZ5SyV"
      },
      "outputs": [],
      "source": [
        "visualize_polygons(p0,p1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsqPNsS4tcZz"
      },
      "source": [
        "### Problem 1a)\n",
        "\n",
        "Please fill in all code blocks marked with a #TODO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHQygwY0pcDl"
      },
      "outputs": [],
      "source": [
        "def threshold_activation1(x):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    y = np.copy(x)\n",
        "    y[y <= 0] = 0\n",
        "    y[y > 0] = 1\n",
        "    return y\n",
        "\n",
        "\n",
        "def and_gate(x):\n",
        "    \"\"\"\n",
        "        x: np array of shape (n, 1)\n",
        "        return: 1 if all elements of x are 1, else 0\n",
        "    \"\"\"\n",
        "    for i in x:\n",
        "        if i <= 0:\n",
        "            return 0\n",
        "\n",
        "    return 1\n",
        "\n",
        "\n",
        "def or_gate(x):\n",
        "    \"\"\"\n",
        "        x: tuple value from AND gates\n",
        "    \"\"\"\n",
        "    if x[0] <= 0 and x[1] <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "def analytical_parameters(p0, p1):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    ## Dimensionality\n",
        "    x_dim = 2\n",
        "    class_num = 2\n",
        "    hidden_unit_num = 10\n",
        "    # First Layer Parameter\n",
        "    W = np.zeros((hidden_unit_num, x_dim))\n",
        "    b = np.zeros((hidden_unit_num, 1))\n",
        "    for i in range(5):\n",
        "        # First polygon\n",
        "        x1 = p0[i, 0]\n",
        "        y1 = p0[i, 1]\n",
        "        x2 = p0[(i+1)%5, 0]\n",
        "        y2 = p0[(i+1)%5, 1]\n",
        "        W[i, :] = [y1 - y2, x2 - x1]\n",
        "        b[i, :] = x1 * y2 - x2 * y1\n",
        "        # Second polygon\n",
        "        x1 = p1[i, 0]\n",
        "        y1 = p1[i, 1]\n",
        "        x2 = p1[(i+1)%5, 0]\n",
        "        y2 = p1[(i+1)%5, 1]\n",
        "        W[i + 5, :] = [y1 - y2, x2 - x1]\n",
        "        b[i + 5, :] = x1 * y2 - x2 * y1\n",
        "    return W,b\n",
        "\n",
        "def predict_output_v1(X, W, b):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for idx in range(data.shape[0]):\n",
        "        x = np.reshape(X[idx, :], (2, 1))\n",
        "        # First layer\n",
        "        first_layer_output = np.matmul(W, x) + b\n",
        "        first_layer_output = threshold_activation1(first_layer_output)\n",
        "        # Second layer\n",
        "        first_polygon = first_layer_output[0:5, :]\n",
        "        second_polygon = first_layer_output[5:10, :]\n",
        "        first_gate_output = and_gate(first_polygon)\n",
        "        second_gate_output = and_gate(second_polygon)\n",
        "        # Output layer\n",
        "        input_to_final_gate = [first_gate_output, second_gate_output]\n",
        "        prediction = or_gate(input_to_final_gate)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "def predict_output_v2(X, W, b):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    ## Cache of Predictions\n",
        "    predictions = []\n",
        "    ## Cycle Through Data Points\n",
        "    for idx in range(data.shape[0]):\n",
        "        x = np.reshape(X[idx, :], (2, 1))\n",
        "        # First layer\n",
        "        first_layer_output = np.matmul(W, x) + b\n",
        "        first_layer_output = threshold_activation1(first_layer_output)\n",
        "        # Second layer\n",
        "        first_polygon = first_layer_output[0:5, :]\n",
        "        second_polygon = first_layer_output[5:10, :]\n",
        "        first_gate_output = and_gate(first_polygon)\n",
        "        second_gate_output = and_gate(second_polygon)\n",
        "        # Output layer\n",
        "        input_to_final_gate = [first_gate_output, second_gate_output]\n",
        "        prediction = first_gate_output\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "def calc_accuracy(true_y, pred_y):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    true_prediction_num = 0\n",
        "    for i, py in enumerate(pred_y):\n",
        "        if py == true_y[i]:\n",
        "            true_prediction_num += 1\n",
        "    accuracy = true_prediction_num / len(pred_y)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSnNFYhu0gDC"
      },
      "source": [
        "*Sanity check:* If you correctly implemented the 'and gate' and 'or gate', all points should be classified correctly when you make predictions using `predict_output_v1()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pknSuuIXrG7Y"
      },
      "outputs": [],
      "source": [
        "## Load Our Parameters\n",
        "W, b = analytical_parameters(p0, p1)\n",
        "\n",
        "## Make Predictions\n",
        "pred_Y = predict_output_v1(X, W, b)\n",
        "\n",
        "## Compute Accuracy\n",
        "acc = calc_accuracy(Y, pred_Y)\n",
        "assert (acc == 1)\n",
        "\n",
        "## Visualize Predictions\n",
        "visualize_datapoints(X, np.array(pred_Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME0TSvkX2KWg"
      },
      "source": [
        "In the code above, change the gates in `predict_output_v2()` such that only the points in the top polygon are classified correctly. Visualize your result, report the accuracy of this model, and attach it to the submission.\n",
        "\n",
        "To further clarify, you should **only** change the usage of the gating functions, not the code inside the gating function itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szTyPVIp1nXq"
      },
      "outputs": [],
      "source": [
        "## Load Our Parameters\n",
        "W, b = analytical_parameters(p0, p1)\n",
        "\n",
        "## Make Predictions\n",
        "pred_Y = predict_output_v2(X, W, b)\n",
        "\n",
        "## Visualize Predictions\n",
        "visualize_datapoints(X, np.array(pred_Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "216MojZrPDFJ"
      },
      "source": [
        "## Problem 1b-d)\n",
        "\n",
        "Complete problems 1b through 1d in the space below. Please use markdown to clearly distinguish your answers for each part. Include appropriate visualizations generated here in your final report."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1.b"
      ],
      "metadata": {
        "id": "-lhN2iI6K809"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# problem 1.b\n",
        "\n",
        "\n",
        "class MLP1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.first_layer = nn.Linear(2, 10)\n",
        "    self.second_layer = nn.Linear(10, 2)\n",
        "    self.output = nn.Linear(2, 1)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "    for layer in [self.first_layer, self.second_layer, self.output]:\n",
        "      nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.activation(self.first_layer(x))\n",
        "    x = self.activation(self.second_layer(x))\n",
        "    x = self.activation(self.output(x))\n",
        "\n",
        "    return x\n",
        ""
      ],
      "metadata": {
        "id": "d6t58-zYL1Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_hat, y):\n",
        "  prediction = (y_hat > 0.5).float()\n",
        "  return (prediction == y).float().mean().item()"
      ],
      "metadata": {
        "id": "2SVPE19jL3Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOMhe2lu7mC5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train(model, x, y, test_x, test_y, loss_function, accuracy_function, batch_size = 128, max_epochs = 500, lr = 0.01):\n",
        "  loss_function = nn.BCELoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "  if isinstance(x, np.ndarray):\n",
        "    x = torch.FloatTensor(x)\n",
        "\n",
        "  if isinstance(y, np.ndarray):\n",
        "    y = torch.FloatTensor(y)\n",
        "\n",
        "  if isinstance(test_x, np.ndarray):\n",
        "    test_x = torch.FloatTensor(test_x)\n",
        "\n",
        "  if isinstance(test_y, np.ndarray):\n",
        "    test_y = torch.FloatTensor(test_y)\n",
        "\n",
        "  if batch_size is None:\n",
        "    batch_size = len(x)\n",
        "\n",
        "  dataset = torch.utils.data.TensorDataset(x, y)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  test_accuracy = []\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for batch_x, batch_y in dataloader:\n",
        "\n",
        "      y_hat = model(batch_x)\n",
        "      loss = loss_function(y_hat, batch_y)\n",
        "      acc = accuracy_function(y_hat, batch_y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc\n",
        "\n",
        "    accuracies.append(epoch_acc / len(dataloader))\n",
        "    losses.append(epoch_loss / len(dataloader))\n",
        "\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    avg_acc = epoch_acc / len(dataloader)\n",
        "\n",
        "    # evaluation\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      test_prediction = (model(test_x) > 0.5).float()\n",
        "      test_acc = (test_prediction.eq(test_y).sum().item()) / len(test_y)\n",
        "      test_accuracy.append(test_acc)\n",
        "\n",
        "\n",
        "    if epoch % 100 == 0 or epoch == max_epochs - 1:\n",
        "\n",
        "      print(\"iteration\",epoch,\"loss:\", avg_loss, \"accuracy:\",avg_acc)\n",
        "\n",
        "\n",
        "\n",
        "  fig0=plt.figure(0)\n",
        "  plt.plot(losses,'-')\n",
        "  plt.xlabel('Iteration', fontsize=18)\n",
        "  plt.ylabel('Loss', fontsize=16)\n",
        "  plt.show()\n",
        "  fig1=plt.figure(1)\n",
        "  plt.plot(accuracies,'-')\n",
        "  plt.xlabel('Iteration', fontsize=18)\n",
        "  plt.ylabel('Accuracy', fontsize=16)\n",
        "  plt.show()\n",
        "\n",
        "  return losses, accuracies, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = X.mean(axis = 0)\n",
        "std = X.std(axis = 0)\n",
        "x_norm = (X - mean) / std\n",
        "\n",
        "\n",
        "x_train, x_test = x_norm[:50000], x_norm[50000:]\n",
        "y_train, y_test = Y[:50000], Y[50000:]\n",
        "\n",
        "\n",
        "X_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "seeds = [0, 1, 2, 3, 4]\n",
        "results = []\n",
        "\n",
        "\n",
        "train_accuracy, test_accuracy = [], []\n",
        "\n",
        "for seed in seeds:\n",
        "  torch.manual_seed(seed)\n",
        "  model = MLP1()\n",
        "\n",
        "  losses, train_accs, test_accs = train(\n",
        "        model,\n",
        "        X_train, Y_train,\n",
        "        X_test, Y_test,\n",
        "        nn.BCELoss(),\n",
        "        accuracy,\n",
        "        batch_size=128,\n",
        "        max_epochs=500,\n",
        "        lr=0.01\n",
        "    )\n",
        "  results.append((losses, train_accs, test_accs))\n",
        "\n",
        "\n",
        "final_train_accs = [r[1][-1] for r in results]\n",
        "final_test_accs = [r[2][-1] for r in results]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS - Problem 1c\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Mean Train Accuracy: {np.mean(final_train_accs):.4f} ± {np.std(final_train_accs):.4f}\")\n",
        "print(f\"Mean Test Accuracy:  {np.mean(final_test_accs):.4f} ± {np.std(final_test_accs):.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nIndividual Seed Results:\")\n",
        "for i, (train_acc, test_acc) in enumerate(zip(final_train_accs, final_test_accs)):\n",
        "    print(f\"Seed {i}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = (model(torch.FloatTensor(x_test)) > 0.5).numpy().astype(int).flatten()\n",
        "\n",
        "visualize_datapoints(x_test * std + mean, preds)\n"
      ],
      "metadata": {
        "id": "KbTLZxQS7L5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MLP2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "            nn.Linear(2, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(8, 4),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(4, 1),\n",
        "            nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    for layer in self.layers:\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        ""
      ],
      "metadata": {
        "id": "nt76cFudMC1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = X.mean(axis = 0)\n",
        "std = X.std(axis = 0)\n",
        "x_norm = (X - mean) / std\n",
        "\n",
        "\n",
        "x_train, x_test = x_norm[:50000], x_norm[50000:]\n",
        "y_train, y_test = Y[:50000], Y[50000:]\n",
        "\n",
        "\n",
        "X_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "seeds = [0, 1, 2, 3, 4]\n",
        "results = []\n",
        "\n",
        "\n",
        "train_accuracy, test_accuracy = [], []\n",
        "\n",
        "for seed in seeds:\n",
        "  torch.manual_seed(seed)\n",
        "  model = MLP2()\n",
        "\n",
        "  losses, train_accs, test_accs = train(\n",
        "        model,\n",
        "        X_train, Y_train,\n",
        "        X_test, Y_test,\n",
        "        nn.BCELoss(),\n",
        "        accuracy,\n",
        "        batch_size=128,\n",
        "        max_epochs=500,\n",
        "        lr=0.001\n",
        "    )\n",
        "  results.append((losses, train_accs, test_accs))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "for i, (losses, train_accs, test_accs) in enumerate(results):\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(losses, label=f'Seed {i}')\n",
        "  plt.title('Training Loss')\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(test_accs, label=f'Seed {i}')\n",
        "  plt.title('Test Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = (model(torch.FloatTensor(x_test)) > 0.5).numpy().astype(int).flatten()\n",
        "\n",
        "visualize_datapoints(x_test * std + mean, preds)  # rescale back to original coords\n"
      ],
      "metadata": {
        "id": "7Tk2QWa-MLxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_train_accs = [r[1][-1] for r in results]\n",
        "final_test_accs = [r[2][-1] for r in results]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS - Problem 1c\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Mean Train Accuracy: {np.mean(final_train_accs):.4f} ± {np.std(final_train_accs):.4f}\")\n",
        "print(f\"Mean Test Accuracy:  {np.mean(final_test_accs):.4f} ± {np.std(final_test_accs):.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nIndividual Seed Results:\")\n",
        "for i, (train_acc, test_acc) in enumerate(zip(final_train_accs, final_test_accs)):\n",
        "    print(f\"Seed {i}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ccxzmYgp5_4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sxSgG9wPDFK"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "All code for Problem 2 should go below. We provide data loaders and relevant imports to get you started. If you are working locally (instead of using Google Colab), we recommend using Conda to install pytorch (https://pytorch.org).\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcocBj9hPDFK"
      },
      "outputs": [],
      "source": [
        "## Additional External Libraries (Deep Learning)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as tfs\n",
        "from PIL import Image\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKYtXumUPDFK"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGEgukvLPDFK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter (Feel Free to Change These, but Make Sure your Training Loop Still Works as Expected)\n",
        "TRAIN_BATCH_SIZE = 50\n",
        "VAL_BATCH_SIZE = 50\n",
        "TEST_BATCH_SIZE = 1\n",
        "\n",
        "# Transform data to PIL images\n",
        "transforms = tfs.Compose([tfs.ToTensor()])\n",
        "\n",
        "# Train/Val Subsets\n",
        "train_mask = range(50000)\n",
        "val_mask = range(50000, 60000)\n",
        "\n",
        "# Download/Load Dataset\n",
        "train_dataset = FashionMNIST('./data', train=True, transform=transforms, download=True)\n",
        "test_dataset = FashionMNIST('./data', train=False, transform=transforms, download=True)\n",
        "\n",
        "# Data Loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, sampler=SubsetRandomSampler(train_mask))\n",
        "val_dataloader = DataLoader(train_dataset, batch_size=VAL_BATCH_SIZE, sampler=SubsetRandomSampler(val_mask))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqfCSiCdPDFK"
      },
      "source": [
        "## Problem 2a)\n",
        "\n",
        "### Design Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWczeepjPDFK"
      },
      "outputs": [],
      "source": [
        "class CNNet(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        ## Inherent Torch Module\n",
        "        super(CNNet, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64 * 3 * 3, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.flatten(start_dim = 1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvczCx2ZPDFK"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgepUwPrPDFK"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          optimizer,\n",
        "          loss,\n",
        "          lr,\n",
        "          epochs=50,\n",
        "          train_dataloader=train_dataloader,\n",
        "          val_dataloader=val_dataloader,\n",
        "          test_dataloader=test_dataloader,\n",
        "          **kwargs):\n",
        "\n",
        "\n",
        "  model = model.cuda()\n",
        "\n",
        "  train_losses, train_accuracies, val_accuracies = [], [], []\n",
        "  best_validation_acc = 0\n",
        "  best_model = None\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        t_correct = 0\n",
        "        t_total = 0\n",
        "\n",
        "        for inputs, labels in train_dataloader:\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            l = loss(outputs, labels)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += l.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            t_total += labels.size(0)\n",
        "            t_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_dataloader)\n",
        "        train_acc = 100 * t_correct / t_total\n",
        "        train_losses.append(avg_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for val_inputs, val_labels in val_dataloader:\n",
        "                val_inputs = val_inputs.cuda()\n",
        "                val_labels = val_labels.cuda()\n",
        "\n",
        "                outputs = model(val_inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += val_labels.size(0)\n",
        "                val_correct += (predicted == val_labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "\n",
        "        # saving best model\n",
        "        if val_acc > best_validation_acc:\n",
        "          best_validation_acc = val_acc\n",
        "          best_model = model.state_dict().copy()\n",
        "\n",
        "  model.load_state_dict(best_model)\n",
        "\n",
        "  # training accuracy on best bodel\n",
        "\n",
        "  model.eval()\n",
        "  final_train_correct = 0\n",
        "  final_train_total = 0\n",
        "  with torch.no_grad():\n",
        "    for train_inputs, train_labels in train_dataloader:\n",
        "      train_inputs, train_labels = train_inputs.cuda(), train_labels.cuda()\n",
        "      outputs = model(train_inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      final_train_total += train_labels.size(0)\n",
        "      final_train_correct += (predicted == train_labels).sum().item()\n",
        "\n",
        "  final_train_acc = 100 * final_train_correct / final_train_total\n",
        "\n",
        "\n",
        "  # testing\n",
        "  model.eval()\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for test_inputs, test_labels in test_dataloader:\n",
        "      test_inputs, test_labels = test_inputs.cuda(), test_labels.cuda()\n",
        "      outputs = model(test_inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      test_total += test_labels.size(0)\n",
        "      test_correct += (predicted == test_labels).sum().item()\n",
        "\n",
        "  test_acc = 100 * test_correct / test_total\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(f'\\n{\"=\"*50}')\n",
        "  print(\"FINAL RESULTS\")\n",
        "  print(f\"{'='*50}\")\n",
        "  print(f\"Training Accuracy: {final_train_acc:.2f}%\")\n",
        "  print(f\"Validation Accuracy: {best_validation_acc:.2f}%\")\n",
        "  print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "  print(f\"{'='*50}\")\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_losses, 'b-', linewidth=2)\n",
        "  plt.title('Training')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.grid(True, alpha=0.3)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(val_accuracies, 'g-', linewidth=2, label='Validation Accuracy')\n",
        "  plt.plot(train_accuracies, 'r--', alpha=0.7, label='Training Accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy (%)')\n",
        "  plt.legend()\n",
        "  plt.grid(True, alpha=0.3)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  return train_losses, train_accuracies, val_accuracies, final_train_acc, best_validation_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmV08pS6PDFK"
      },
      "outputs": [],
      "source": [
        "## Hyperparameters\n",
        "EPOCH = 50\n",
        "LR = 0.01\n",
        "\n",
        "## Setting up the model, optimizer, and loss function\n",
        "model = CNNet()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_f = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnmWFEt3PDFK"
      },
      "outputs": [],
      "source": [
        "## Run Training Loop\n",
        "out = train(model, optimizer, loss_f, LR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPN1uK8KPDFL"
      },
      "source": [
        "## Problem 2b)\n",
        "\n",
        "Now try to improve your model using additional techniques learned during class. You should be able to use the same training function as above, but will need to create a new model architecture.\n",
        "\n",
        "### Data Loading\n",
        "\n",
        "You should maintain the splits from above, but feel free to alter the dataloaders (i.e. transforms) as you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar110CBLPDFL"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter (Feel Free to Change These, but Make Sure your Training Loop Still Works as Expected)\n",
        "TRAIN_BATCH_SIZE = 50\n",
        "VAL_BATCH_SIZE = 50\n",
        "TEST_BATCH_SIZE = 1\n",
        "\n",
        "# Transform data to PIL images\n",
        "transforms = tfs.Compose([\n",
        "    tfs.ToTensor(),\n",
        "    tfs.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] range\n",
        "])\n",
        "\n",
        "# Train/Val Subsets\n",
        "train_mask = range(50000)\n",
        "val_mask = range(50000, 60000)\n",
        "\n",
        "# Download/Load Dataset\n",
        "train_dataset = FashionMNIST('./data', train=True, transform=transforms, download=True)\n",
        "test_dataset = FashionMNIST('./data', train=False, transform=transforms, download=True)\n",
        "\n",
        "# Data Loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, sampler=SubsetRandomSampler(train_mask))\n",
        "val_dataloader = DataLoader(train_dataset, batch_size=VAL_BATCH_SIZE, sampler=SubsetRandomSampler(val_mask))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8F7EIxXPDFL"
      },
      "source": [
        "### Model Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYa6pKlmPDFL"
      },
      "outputs": [],
      "source": [
        "class CNNImproved(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNImproved, self).__init__()\n",
        "    self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "    self.fc_layers = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128 * 3 * 3, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-vJ08i8PDFL"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNImproved()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "out = train(model, optimizer, loss_f, LR)"
      ],
      "metadata": {
        "id": "lJVbsa8dmFzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaGzm3x7PDFL"
      },
      "source": [
        "## Problem 2c)\n",
        "\n",
        "Write down your response in the final report."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I did a bit of research in Geoffery Hilton's Distilling the Knowledge in a Neural Netowrk (2015). This technique is as follows. A large neural network (the teacher) is trained on the FashionMNIST dataset to achieve a high accuracy. Its outputs are stored as the \"knowledge\" and soften using temperature T > 1, a small change to the softmax function. Then, a small network (the student) is with fewer layers is trained. However, instead of learning from the true dataset, it uses the soften outputs from the teacher. This technique allows for the student to be a much smaller neural network, but can achieve high accuracy because it leverages the teacher's knowledge.\n"
      ],
      "metadata": {
        "id": "uWltMNhkwQeE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}