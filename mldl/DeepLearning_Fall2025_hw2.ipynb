{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06bnDgIYNrim"
   },
   "source": [
    "# Homework 2: Programming\n",
    "\n",
    "The following notebook contains skeleton-code for answering problems 2 and 3 of homework assignment 2. Please read through each cell carefully to understand what is expected to be implemented. For your final submission, please try to clean up any intermediate outputs used for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scoOJkUyNrin"
   },
   "source": [
    "### Imports\n",
    "\n",
    "You should be able to complete the entire assignment using only the following imports. Please consult the course staff if you are unsure about whether additional packages may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1692746569066,
     "user": {
      "displayName": "Ping-Cheng Ku",
      "userId": "13760135399731224207"
     },
     "user_tz": 300
    },
    "id": "9vllQSx4E9oP"
   },
   "outputs": [],
   "source": [
    "## Import Packages\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJX0u5W1cZDc"
   },
   "source": [
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGn3Fryvfrbu"
   },
   "source": [
    "Below we provide an AutoGrad class named `Value`. The basic idea is to store the existing computational map during the creation of each `Value`, and calculate the gradient using backpropagation when one of the `Value` calls `backward()` method.\n",
    "\n",
    "The `backward()` function will arange the computational graph and backpropagate the gradients. All you need to do is to implement all the operations with its corresponding `_backward` function. We have provided the `__add__` function (sum of two nodes) as an example to help get you started.\n",
    "\n",
    "This notebook is designed in a Object Oriented way, if you are not farmiliar with the Object Oriented Programming in Python, you can refer to:\n",
    "\n",
    "(1) https://realpython.com/python3-object-oriented-programming/\n",
    "\n",
    "(2) https://docs.python.org/3/tutorial/classes.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7saLrsq-5Wc6"
   },
   "outputs": [],
   "source": [
    "from numpy import e\n",
    "\n",
    "\n",
    "class Value:\n",
    "\n",
    "    \"\"\"\n",
    "    Basic unit of storing a single scalar value and its gradient\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=()):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        \"\"\"computing greater than or equal to\"\"\"\n",
    "\n",
    "        if isinstance(other, Value):\n",
    "            return self.data >= other.data\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Example implementation of a single class operation (addition)\n",
    "\n",
    "        Args:\n",
    "            other (Any): Node to add with the class\n",
    "\n",
    "        Returns:\n",
    "            out (callable): Function to referesh the gradient\n",
    "        \"\"\"\n",
    "        #Firstly, convert some default value type in python to Value\n",
    "        #Then do operations with two or more Value object\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        #Secondly, create a new Value object which is the result of the operation\n",
    "        out = Value(self.data + other.data, (self, other))\n",
    "\n",
    "        #Thirdly, create a _backward function for the output object to refresh\n",
    "        # the gradient of its _childrens,\n",
    "        #Then assign this _backward function to the output object.\n",
    "        def _backward():\n",
    "            self.grad += out.grad * 1.0\n",
    "            other.grad += out.grad * 1.0\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        Multiplication operation (e.g. Value(3) * Value(2) = Value(6))\n",
    "        \"\"\"\n",
    "        \n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(other.data * self.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        \"\"\"\n",
    "        Power operation (e.g Value(3) ** 2 = Value(9))\n",
    "        \"\"\"\n",
    "        assert isinstance(other, (int, float))\n",
    "\n",
    "        out = Value(self.data ** other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other * (self.data ** (other - 1))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        ReLU activation function applied to the current Value\n",
    "        \"\"\"\n",
    "        out = Value(max(0, self.data), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            if self.data > 0:\n",
    "                self.grad += out.grad * 1.0\n",
    "            else:\n",
    "                self.grad += out.grad * 0.0\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Exponentiate the current Value (e.g. e ^ Value(0) = Value(1))\n",
    "        \"\"\"\n",
    "        out = Value(np.exp(self.data), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (np.exp(self.data))\n",
    "        out._backward = _backward\n",
    "        return out \n",
    "\n",
    "    def log(self):\n",
    "        \"\"\"\n",
    "        Take the natural logarithm (base e) of the current Value\n",
    "        \"\"\"\n",
    "\n",
    "        out = Value(np.log(self.data), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1.0 / self.data)\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Run backpropagation from the current Value\n",
    "        \"\"\"\n",
    "        #This function is called when you start backpropagation from this Value\n",
    "        \n",
    "\n",
    "        #The gradient of this value is initialized to 1 for you.\n",
    "        self.grad = 1.0\n",
    "\n",
    "        #You need to find a right topological order all of the children in the graph.\n",
    "        #As for topology sort, you can refer to http://www.cs.cornell.edu/courses/cs312/2004fa/lectures/lecture15.htm\n",
    "\n",
    "        topo = []\n",
    "        visited = set()  # a set containing all nodes waiting to be visited\n",
    "\n",
    "\n",
    "        depth = 0\n",
    "        \n",
    "\n",
    "        def visit(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "\n",
    "                nonlocal depth\n",
    "                if depth > 1000:\n",
    "                    print(\"crying\")\n",
    "\n",
    "                depth += 1\n",
    "\n",
    "                for child in node._prev:\n",
    "                    visit(child)\n",
    "\n",
    "                topo.append(node) # adding current node to the head\n",
    "\n",
    "        visit(self)\n",
    "\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "    \n",
    "\n",
    "    # We handled the negation and reverse operations for you\n",
    "    def __neg__(self): # -self\n",
    "        \"\"\"\n",
    "        Negate the current Value\n",
    "        \"\"\"\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): #other + self\n",
    "        \"\"\"\n",
    "        Reverse addition operation (ordering matters in Python)\n",
    "        \"\"\"\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        \"\"\"\n",
    "        Subtraction operation\n",
    "        \"\"\"\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        \"\"\"\n",
    "        Reverse subtraction operation\n",
    "        \"\"\"\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        \"\"\"\n",
    "        Reverse multiplication operation\n",
    "        \"\"\"\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        \"\"\"\n",
    "        Division operation\n",
    "        \"\"\"\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        \"\"\"\n",
    "        Reverse diction operation\n",
    "        \"\"\"\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Class representation (instead of unfriendly memory address)\n",
    "        \"\"\"\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWMte8xaeWpK"
   },
   "source": [
    "Now, we are going to use the simple example in q1.b to get you familar with the usage of this class.\n",
    "\n",
    "If your implementation is correct, you will get the same values and gradients as your hand-caculated ones.\n",
    "\n",
    "Be careful! Even you get this test case right, it does not guarantee the correctness of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PeoL0WIL_sHD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = w1 * x1 = 0.2 * -0.4 = -0.0800\n",
      "b = w2 * x2 = 0.4 * 0.5 = 0.2000\n",
      "c = a + b = -0.0800 + 0.2000 = 0.1200\n",
      "d = sigmoid(c) = sigmoid(0.1200) = 0.5300\n",
      "e = w1 ** 2 = 0.2 ** 2 = 0.0400\n",
      "g = w2 ** 2 = 0.4 ** 2 = 0.1600\n",
      "h = e + g = 0.0400 + 0.1600 = 0.2000\n",
      "i = 0.5 * h = 0.5 * 0.2000 = 0.1000\n",
      "f = d + i = 0.5300 + 0.1000 = 0.6300\n",
      "w1.grad: 0.10035913775925998, w2.grad: 0.524551077800925, x1.grad: 0.04982043112037002, x2.grad: 0.09964086224074004\n"
     ]
    }
   ],
   "source": [
    "## Initialize Example Values (From Written Assignment)\n",
    "w1 = Value(0.2)\n",
    "w2 = Value(0.4)\n",
    "x1 = Value(-0.4)\n",
    "x2 = Value(0.5)\n",
    "\n",
    "#TODO\n",
    "#Do calculation for the question 1.b, and call backward to start backpropagation.\n",
    "#Then print out the gradient of w1 w2 x1 x2.\n",
    "\n",
    "a = w1 * x1\n",
    "print(f\"a = w1 * x1 = {w1.data} * {x1.data} = {a.data:.4f}\")\n",
    "\n",
    "b = w2 * x2\n",
    "print(f\"b = w2 * x2 = {w2.data} * {x2.data} = {b.data:.4f}\")\n",
    "c = a + b\n",
    "print(f\"c = a + b = {a.data:.4f} + {b.data:.4f} = {c.data:.4f}\")\n",
    "d = sigmoid(c)\n",
    "print(f\"d = sigmoid(c) = sigmoid({c.data:.4f}) = {d.data:.4f}\")\n",
    "e = w1 ** 2\n",
    "print(f\"e = w1 ** 2 = {w1.data} ** 2 = {e.data:.4f}\")\n",
    "g = w2 ** 2\n",
    "print(f\"g = w2 ** 2 = {w2.data} ** 2 = {g.data:.4f}\")\n",
    "h = e + g\n",
    "print(f\"h = e + g = {e.data:.4f} + {g.data:.4f} = {h.data:.4f}\")\n",
    "i = Value(0.5) * h\n",
    "print(f\"i = 0.5 * h = 0.5 * {h.data:.4f} = {i.data:.4f}\")\n",
    "f = d + i\n",
    "print(f\"f = d + i = {d.data:.4f} + {i.data:.4f} = {f.data:.4f}\")\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f\"w1.grad: {w1.grad}, w2.grad: {w2.grad}, x1.grad: {x1.grad}, x2.grad: {x2.grad}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "x = Value(2.0)\n",
    "y = x ** 3\n",
    "print(y.data)  # Should be 8.0\n",
    "\n",
    "# Test backward pass\n",
    "y.backward()\n",
    "print(x.grad)  # Should be 12.0 (3 * 2^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oowriEHef1b7"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lcv2k0PLg6wY"
   },
   "source": [
    "### Implementation of the linear layer\n",
    "You will implement a `LinearLayer` module here.\n",
    "\n",
    "We provide the initialization of the class `LinearLayer`. You need to implement the forward function -- Return the results - `out` with the shape `[n_samples, n_out_channels]` of a linear layer when the the data `x` shaped `[n_samples, n_in_channels]` is fed into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NTHmcr7pLbU0"
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    \"\"\"\n",
    "    Base Model Module\n",
    "    \"\"\"\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "class LinearLayer(Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Linear Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"\n",
    "        Here we randomly initilize the weights w as 2-dimensional list of Values\n",
    "        And b as 1-dimensional list of Values with value 0\n",
    "\n",
    "        You may use this stucture to implement the __call__ function\n",
    "        \"\"\"\n",
    "        self.w = []\n",
    "        for i in range(nin):\n",
    "            w_tmp = [Value(random.uniform(-1,1)) for j in range(nout)]\n",
    "            self.w.append(w_tmp)\n",
    "        self.b = [Value(0) for i in range(nout)]\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (2d-list): Two dimensional list of Values with shape [batch_size , nin]\n",
    "\n",
    "        Returns:\n",
    "            xout (2d-list): Two dimensional list of Values with shape [batch_size, nout]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = len(x)\n",
    "        xout = []\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            sample_outputs = []\n",
    "            \n",
    "            for j in range(self.nout):\n",
    "                products = [self.w[k][j] * x[i][k] for k in range(self.nin)]\n",
    "                w_sum = sum(products, start=Value(0.0))\n",
    "                sample_outputs.append(w_sum + self.b[j])\n",
    "        \n",
    "        xout.append(sample_outputs)\n",
    "    \n",
    "        return xout\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Get the list of parameters in the Linear Layer\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            params (list): List of parameters in the layer\n",
    "        \"\"\"\n",
    "        return [p for row in self.w for p in row] + [p for p in self.b]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gwYpbiRfoFH"
   },
   "source": [
    "Test your implementation of linear layer, the error should be nearly 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fHLnHUW-fydy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "## Initialization of Layer with Weights\n",
    "linear_model_test = LinearLayer(4, 4)\n",
    "linear_model_test.w = [[Value(data=0.7433570245252463), Value(data=-0.9662164096144394), Value(data=-0.17087204941322653), Value(data=-0.5186656374983067)],\n",
    "                       [Value(data=-0.1414882837892344), Value(data=-0.5898971049017006), Value(data=-0.3448340220492381), Value(data=0.5278833226346107)],\n",
    "                       [Value(data=0.3990701306597799), Value(data=-0.3319058654296163), Value(data=-0.784797384411202), Value(data=0.7603317495966846)],\n",
    "                       [Value(data=-0.5711035064293541), Value(data=-0.0001937643033362857), Value(data=0.12693226232877053), Value(data=-0.36044237239197097)]]\n",
    "linear_model_test.b = [Value(data=0), Value(data=0), Value(data=0), Value(data=0)]\n",
    "\n",
    "## Forward Pass\n",
    "x_test = [[-0.17120438454836173, -0.3736077734087335, -0.48495413054653214, 0.8269206715993096]]\n",
    "y_hat_test = linear_model_test(x_test)\n",
    "y_ref = [[Value(data=-0.7401928625441141), Value(data=0.5466095223360173), Value(data=0.6436403600545564), Value(data=-0.7752067527386406)]]\n",
    "\n",
    "## Error Calculation\n",
    "predict_error = 0\n",
    "for i in range(4):\n",
    "    predict_error += (y_hat_test[0][i] - y_ref[0][i])**2\n",
    "print(predict_error.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfSXiqRShoM1"
   },
   "source": [
    "### Implementation of Loss functions\n",
    "\n",
    "You will implement softmax, cross entropy loss, and accuracy here for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KW_OLYJ_hhbY"
   },
   "outputs": [],
   "source": [
    "def softmax(y_hat):\n",
    "    \"\"\"\n",
    "    Softmax computation\n",
    "\n",
    "    Args:\n",
    "        y_hat (2d-list): 2-dimensional list of Values with shape [batch_size, n_class]\n",
    "\n",
    "    Returns:\n",
    "        s (2d-list): 2-dimensional list of Values with the same shape as y_hat\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(y_hat)\n",
    "    num_classes = len(y_hat[0])\n",
    "\n",
    "    y_hat_exp = [[element.exp() for element in row] for row in y_hat]  # e^s_ij\n",
    "\n",
    "    sum_exp = [sum(row) for row in y_hat_exp] #summation ove each row\n",
    "\n",
    "    s = []\n",
    "    for i in range(batch_size):\n",
    "        row = []\n",
    "        for j in range(num_classes):\n",
    "            prob = y_hat_exp[i][j] / sum_exp[i]\n",
    "            row.append(prob)\n",
    "        s.append(row)\n",
    "\n",
    "    s = [[y_hat_exp[i][j] / sum_exp[i] for j in range(len(y_hat[0]))] for i in range(len(y_hat))] # s_ij = e^s_ij / sum(e^s_ik)\n",
    "\n",
    "\n",
    "    def backward():\n",
    "        for i in range(batch_size):\n",
    "            for j in range(num_classes):\n",
    "                grad = 0.0\n",
    "                for k in range(num_classes):\n",
    "                    if j == k:\n",
    "                        grad += s[i][k].data * (1 - s[i][k].data) * s[i][k].grad\n",
    "                    else:\n",
    "                        grad += -s[i][k].data * s[i][k].data * s[i][k].grad\n",
    "                y_hat[i][j].grad += grad\n",
    "\n",
    "    s[-1][-1]._backward = backward\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9xdTC9Tym6sP"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat, y):\n",
    "    \"\"\"\n",
    "    Cross-entropy Loss computation\n",
    "\n",
    "    Args:\n",
    "        y_hat (2d-list): Output from linear function with shape [batch_size, n_class]\n",
    "        y (1d-list): List of ground truth labels with shape [batch_size, ]\n",
    "\n",
    "    Returns:\n",
    "        loss (Value): Loss value of type Value\n",
    "    \"\"\"\n",
    "\n",
    "    s = softmax(y_hat)  # softmax output\n",
    "\n",
    "    batch_size = len(y_hat)\n",
    "    class_size = len(y_hat[0])\n",
    "\n",
    "    loss = Value(0.0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(class_size): \n",
    "            if j == y[i]:\n",
    "                loss = loss + -(s[i][j].log())  # cross-entropy loss\n",
    "  \n",
    "    loss = loss / Value(batch_size)\n",
    "\n",
    "    def _backward():\n",
    "        for i in range(batch_size):\n",
    "            for j in range(class_size):\n",
    "                if j == y[i]:\n",
    "                    y_hat[i][j].grad += (s[i][j] - 1.0) / batch_size\n",
    "                else:\n",
    "                    y_hat[i][j].grad += s[i][j] / batch_size\n",
    "\n",
    "    loss._backward = _backward\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8jeK31K85qsB"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"\n",
    "    Accuracy computation\n",
    "\n",
    "    Args:\n",
    "        y_hat (2d-list): Output from linear function with shape [batch_size, n_class]\n",
    "        y (1d-list): List of ground truth labels with shape [batch_size, ]\n",
    "\n",
    "    Returns:\n",
    "        acc (float): Accuracy score\n",
    "    \"\"\"\n",
    "    batch_size = len(y_hat)\n",
    "    class_size = len(y_hat[0])\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        predicted_class = 0\n",
    "        max_prob = y_hat[i][0].data\n",
    "        \n",
    "\n",
    "        for j in range(class_size):\n",
    "            temp = y_hat[i][j].data\n",
    "            max_btwn = np.maximum(max_prob, y_hat[i][j].data)\n",
    "            if max_btwn == y_hat[i][j].data:\n",
    "                max_prob = y_hat[i][j].data\n",
    "                predicted_class = j\n",
    "        \n",
    "        true_class = y[i]\n",
    "        \n",
    "        if predicted_class == true_class:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / batch_size\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jti1kRlzf14E"
   },
   "source": [
    "Test the implementation of `softmax()` and `cross_entropy_loss()` as well as the gradient calculation of `Value` class. The errors should be nearly 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "My_zbUj7gBLo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "Value(data=1.1086134242379026e-32, grad=0.0)\n",
      "Value(data=1.3866695599588098e-32, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "## Ground Truth + Forward Pass\n",
    "y_gt = [1]\n",
    "y_hat_test = linear_model_test(x_test)\n",
    "\n",
    "## Softmax Calculation\n",
    "prob_test = softmax(y_hat_test)\n",
    "prob_ref = [[0.10441739448437284, 0.37811510516540814, 0.4166428991676558, 0.10082460118256342]]\n",
    "softmax_error = 0\n",
    "for i in range(4):\n",
    "    softmax_error += (prob_ref[0][i] - prob_test[0][i])**2\n",
    "print(softmax_error.data)\n",
    "\n",
    "## Cross Entropy Loss Calculation\n",
    "loss_test = cross_entropy_loss(y_hat_test, y_gt)\n",
    "loss_ref = Value(data=0.9725566186970217)\n",
    "print((loss_test - loss_ref).data)\n",
    "\n",
    "## Update Gradient Based on Loss\n",
    "linear_model_test.zero_grad()\n",
    "loss_test.backward()\n",
    "w_gradient_ref = [[-0.017876715758840547, 0.10646942068007896, -0.07133109112844363, -0.01726161379279479],\n",
    "                  [-0.0390111502584479, 0.23234103087567629, -0.1556610258645873, -0.03766885475264107],\n",
    "                  [-0.05063764675610328, 0.30158564847453107, -0.2020526949142369, -0.04889530680419089],\n",
    "                  [0.08634490197366762, -0.5142494748940867, 0.3445306259968013, 0.08337394692361787]]\n",
    "b_gradient_ref = [0.10441739448437282, -0.6218848948345919, 0.4166428991676557, 0.1008246011825634]\n",
    "\n",
    "## Compute Error\n",
    "w_gradient_error = 0\n",
    "b_gradient_error = 0\n",
    "for i in range(4):\n",
    "    b_gradient_error += (linear_model_test.b[i].grad - b_gradient_ref[i]) ** 2\n",
    "    for j in range(4):\n",
    "        w_gradient_error += (linear_model_test.w[i][j].grad - w_gradient_ref[i][j]) ** 2\n",
    "print(w_gradient_error)\n",
    "print(b_gradient_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovsoGs3_ljNC"
   },
   "source": [
    "Implement the following functions to visualize the ground truth and the decision boundary in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "B9kPT-caWUaw"
   },
   "outputs": [],
   "source": [
    "def plot_points(X, Y, scale, n, data):\n",
    "    \"\"\"\n",
    "    Plot points in the visualization image\n",
    "    \"\"\"\n",
    "    points_color = [[0., 0. , 255.], [255., 0., 0.], [0., 255., 0.],[0., 0. , 0.]]\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        x1 = X[i,0]\n",
    "        x2 = X[i,1]\n",
    "        label = Y[i]\n",
    "        x_pixel = int((x1 + scale) / (2 * scale) * n)\n",
    "        y_pixel = int((x2 + scale) / (2 * scale) * n)\n",
    "\n",
    "        if 0 <= x_pixel < n and 0 <= y_pixel < n: # if value pixel     \n",
    "            data[y_pixel, x_pixel] = points_color[label]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_background(scale, n, model):\n",
    "    \"\"\"\n",
    "    Color the background in the visualization image\n",
    "    \"\"\"\n",
    "\n",
    "    background_color = [[0., 191., 255.], [255., 110., 180.], [202., 255., 112.],[156., 156., 156.]]\n",
    "\n",
    "    data = np.zeros((n,n,3), dtype='uint8')\n",
    "\n",
    "    for i in range(n):\n",
    "        x1 = -scale + 2 * scale / n * i\n",
    "        for j in range(n):\n",
    "            x2 = -scale + 2 * scale / n * j\n",
    "            input = [[Value(x1),Value(x2)]]\n",
    "\n",
    "            prediction = model(input)  # model prediction\n",
    "            label = np.argmax([prediction[0][k].data for k in range(len(prediction[0]))])\n",
    "            data[i, j] = background_color[label]\n",
    "    return data\n",
    "\n",
    "\n",
    "def visualization(X, Y, model):\n",
    "    \"\"\"\n",
    "    Decision boundary visualization\n",
    "    \"\"\"\n",
    "    scale = 4.5  # the scale of X axis and Y axis. To say, x is from -scale to +scale\n",
    "    n = 300      # seperate the image into n*n pixels\n",
    "\n",
    "    data = plot_background (scale, n, model)\n",
    "    data = plot_points (X, Y, scale, n, data)\n",
    "\n",
    "    plt.imshow(data)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRJgvFM6Ob-V"
   },
   "source": [
    "if you implement the plot function correctly, you will get some image like:\n",
    "\n",
    "![download.png](data:image/png;base64,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)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_value(obj):\n",
    "    \"\"\"Recursively extract the numerical value from nested Value objects\"\"\"\n",
    "    current = obj\n",
    "    while hasattr(current, 'data'):\n",
    "        val = current.data\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaPMBVcZiVah"
   },
   "source": [
    "### Implementation of training procedure\n",
    "\n",
    "With input data `x`, ground_truth `y`, and `model` as parameters, implement the gradient descent method to train your model and plot loss and accuracy vs training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "d9b1UnDTO12e"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def train(x,\n",
    "          y,\n",
    "          model,\n",
    "          loss_function=cross_entropy_loss,\n",
    "          accuracy_function=accuracy,\n",
    "          max_iteration=500,\n",
    "          learning_rate=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       x (2-d list): List of Values with shape: [n_samples, n_channels]\n",
    "       y (1-d list): List of integers with shape: [n_samples]\n",
    "       model (Module): Linear model\n",
    "       loss_function (callable): Loss function to use during training\n",
    "       accuracy_function (callable): Function used for calculating training accuracy\n",
    "       max_iteration (int): Number of epochs to train model for\n",
    "       learning_rate (numeric): Step size of the gradient update\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for i in range(max_iteration):\n",
    "        y_hat = model(x)\n",
    "        loss = loss_function(y_hat, y)\n",
    "        acc = accuracy_function(y_hat, y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for parameter in model.parameters():\n",
    "            parameter.data -= learning_rate * parameter.grad\n",
    "            parameter.grad = 0.0        \n",
    "\n",
    "\n",
    "        #Then plot the loss / accuracy vs iterations.\n",
    "        if i % 20 == 19:\n",
    "            print(\"iteration\",i,\"loss:\",(extract_final_value(loss)), \"accuracy:\",acc)\n",
    "\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            model.zero_grad()\n",
    "            gc.collect()\n",
    "        \n",
    "        ## record loss\n",
    "        if i == 0 :\n",
    "        # initialize L\n",
    "            L = loss.data\n",
    "            A = acc\n",
    "        else:\n",
    "            L = np.append(L,loss.data)\n",
    "            A = np.append(A,acc)\n",
    "\n",
    "    ## Plot Loss and Accuracy\n",
    "    fig0=plt.figure(0)\n",
    "    plt.plot(L,'-')\n",
    "    plt.xlabel('Iteration', fontsize=18)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.show()\n",
    "    fig1=plt.figure(1)\n",
    "    plt.plot(A,'-')\n",
    "    plt.xlabel('Iteration', fontsize=18)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9p8QtQJjeAl"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Load the data, format it, instantiate your model and start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOHM_FgtoUyu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,) [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "## Load Q3 Dataset\n",
    "\n",
    "svpath = 'HW2_Q3_dataset/Q3_data.npz'\n",
    "data = np.load(svpath)\n",
    "\n",
    "# X, array of shape [n_samples, n_features]\n",
    "# Y, array of shape [n_samples]\n",
    "X = data['X']\n",
    "Y = data['Y']\n",
    "print(X.shape, Y.shape, np.unique(Y))\n",
    "nin = X.shape[1]\n",
    "nout = np.max(Y) + 1\n",
    "\n",
    "## Initialize data using your Value class\n",
    "x = [[Value(v) for v in sample] for sample in X]\n",
    "y = [int(v) for v in Y]\n",
    "\n",
    "## Initialize a Linear Model\n",
    "linear_model = LinearLayer(nin, nout)\n",
    "\n",
    "## Train the Model using Your Data\n",
    "train(x, y, linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEUUcgHEM6of"
   },
   "outputs": [],
   "source": [
    "## Visualize learned decision boundaries\n",
    "visualization(X, Y, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctXkntnIkdjt"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFoPi3NskoIh"
   },
   "source": [
    "### a) Is this dataset linear separable?\n",
    "load the dataset for this question and train a linear model on this dataset and report the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1BHFxm4Ix2c"
   },
   "outputs": [],
   "source": [
    "## Load Q4 Dataset\n",
    "svpath = 'HW2_Q4_dataset/Q4_data.npz'\n",
    "data = np.load(svpath)\n",
    "\n",
    "## Parse Data and Identify Dimensions\n",
    "X = data['X']\n",
    "Y = data['Y']\n",
    "nin = X.shape[1]\n",
    "nout = int(np.max(Y)) + 1\n",
    "\n",
    "## Initialize data using your value class\n",
    "x = [[Value(v) for v in sample] for sample in X]\n",
    "y = [int(v) for v in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlFvA3CCS33X"
   },
   "outputs": [],
   "source": [
    "## Initialize Linear Model\n",
    "linear_model = LinearLayer(nin, nout)\n",
    "\n",
    "## Train Model\n",
    "train(x, y, linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cFD2eQ2WWUQ"
   },
   "outputs": [],
   "source": [
    "## Visualize Learned Decision Boundary\n",
    "visualization(X, Y, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYoFsdm0m18J"
   },
   "source": [
    "### b) Implementation of Multi Layer Perceptron (MLP)\n",
    "\n",
    "Implement a class `MLP` to add arbitrary layers. You will need to implement the forward function to return results `out` with `x` fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldxalUrYLOZW"
   },
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Multi Layer Perceptron\n",
    "    \"\"\"\n",
    "    def __init__(self, dimensions):\n",
    "        \"\"\"\n",
    "        Initialize multiple layers here in the list named self.linear_layers\n",
    "        \"\"\"\n",
    "        assert isinstance(dimensions, list)\n",
    "        assert len(dimensions) > 2\n",
    "        self.linear_layers = []\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            self.linear_layers.append(LinearLayer(dimensions[i], dimensions[i+1]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (2d-list): Two dimensional list of Values with shape [batch_size , nin]\n",
    "\n",
    "        Returns:\n",
    "            xout (2d-list): Two dimensional list of Values with shape [batch_size, nout]\n",
    "        \"\"\"\n",
    "\n",
    "        output = X\n",
    "        \n",
    "        for i , layer in enumerate(self.linear_layers[:-1]):\n",
    "            output = [layer(sample) for sample in output] # linear transf\n",
    "            output = [[val.relu() for val in sample] for sample in output] # relu\n",
    "\n",
    "        output = [self.linear_layers[-1](sample) for sample in output] # last linear layer without relu\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Get the parameters of each layer\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            params (list of Values): Parameters of the MLP\n",
    "        \"\"\"\n",
    "        return [p for layer in self.linear_layers for p in layer.parameters()]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Zero out the gradient of each parameter\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrFd0boknj94"
   },
   "source": [
    "Train your MLP model and visualize the decision boundary with ground truth points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_zTwbDZQ4I4"
   },
   "outputs": [],
   "source": [
    "## Initialize MLP with Given Parameters\n",
    "mlp_model = MLP([nin, 40, nout])\n",
    "\n",
    "## Train the MLP\n",
    "train(x, y, mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr2-BMquWPg_"
   },
   "outputs": [],
   "source": [
    "## Visualize Decision Boundaries\n",
    "visualization(X, Y, mlp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSoTfLFsau-Q"
   },
   "source": [
    "## Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9tFA4DZa4E6"
   },
   "source": [
    "The design of the auto grade structure are based on the work https://github.com/karpathy/micrograd"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GSoTfLFsau-Q"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
