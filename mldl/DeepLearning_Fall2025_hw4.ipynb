{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3G5iOckPDFC"
      },
      "source": [
        "# Deep Learning (Fall 2025) - Homework 4\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*Developed by Hongtau Wu & Suzanna Sia*\n",
        "\n",
        "This notebook contains all starter code for Homework 4. Please read the written assignment carefully to ensure you include all necessary outputs in your final report. Your final submission (a single zip file) should include this notebook (.ipynb file) and a PDF of this notebook with all cell outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-QLXT-YPDFE"
      },
      "source": [
        "## Problem 1a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgqW4Ud3PDFE"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwpCO80kPDFF"
      },
      "outputs": [],
      "source": [
        "## External Libararies\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "from matplotlib.collections import PatchCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdNgZM70PDFF"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c92eg0MRracO"
      },
      "outputs": [],
      "source": [
        "## Spectify Path to Provided Data Here\n",
        "DATA_PATH = '/home/aappiah7/fall25/mldl/DeepLearning_Fall2025_hw4_prob1_data.npy'\n",
        "\n",
        "## Load Data and Check Dimensionality\n",
        "data = np.load(DATA_PATH)\n",
        "Y = data[:,2]\n",
        "X = data[:,0:2]\n",
        "print(\"Y:\", Y.shape)\n",
        "print(\"X:\", X.shape)\n",
        "\n",
        "## Polygon Boundaries\n",
        "p = [[[500, 1000], [300, 800], [400, 600], [600, 600], [700, 800]],\n",
        "     [[500, 600], [100, 400], [300, 200], [700, 200], [900, 400]]]\n",
        "p = np.asarray(p)\n",
        "p0 = p[0]\n",
        "p1 = p[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw0Qur2lPDFG"
      },
      "source": [
        "### Visualization Code\n",
        "\n",
        "Do not touch any of the visualization code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLtNWWcIvVYk"
      },
      "outputs": [],
      "source": [
        "## Helper code for visualisation (No Need to Touch)\n",
        "def visualize_polygons(p0, p1):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    patches = []\n",
        "    polygon1 = Polygon(p0, closed=True)\n",
        "    polygon2 = Polygon(p1, closed=True)\n",
        "    patches.append(polygon1)\n",
        "    patches.append(polygon2)\n",
        "    p = PatchCollection(patches, cmap=matplotlib.cm.jet, alpha=0.4)\n",
        "    ax.add_collection(p)\n",
        "    ax.autoscale_view()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_datapoints(X, Y):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    assert(X.shape[0] == Y.shape[0])\n",
        "    fig, ax = plt.subplots()\n",
        "    npts = 60000\n",
        "    col = np.where(Y[:npts]==1,'m','b')\n",
        "    x1 = X[:npts][:,0]\n",
        "    x2 = X[:npts][:,1]\n",
        "    ax.scatter(x1, x2, s=0.5, c=col, zorder=1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yDFI7mZ5SyV"
      },
      "outputs": [],
      "source": [
        "visualize_polygons(p0,p1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsqPNsS4tcZz"
      },
      "source": [
        "### Problem 1a)\n",
        "\n",
        "Please fill in all code blocks marked with a #TODO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHQygwY0pcDl"
      },
      "outputs": [],
      "source": [
        "def threshold_activation1(x):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    y = np.copy(x)\n",
        "    y[y <= 0] = 0\n",
        "    y[y > 0] = 1\n",
        "    return y\n",
        "\n",
        "\n",
        "def and_gate(x):\n",
        "    \"\"\"\n",
        "        x: np array of shape (n, 1)\n",
        "        return: 1 if all elements of x are 1, else 0\n",
        "    \"\"\"\n",
        "    for i in x:\n",
        "        if i <= 0:\n",
        "            return 0\n",
        "        \n",
        "    return 1\n",
        "\n",
        "\n",
        "def or_gate(x):\n",
        "    \"\"\"\n",
        "        x: tuple value from AND gates\n",
        "    \"\"\"\n",
        "    if x[0] <= 0 and x[1] <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "def analytical_parameters(p0, p1):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    ## Dimensionality\n",
        "    x_dim = 2\n",
        "    class_num = 2\n",
        "    hidden_unit_num = 10\n",
        "    # First Layer Parameter\n",
        "    W = np.zeros((hidden_unit_num, x_dim))\n",
        "    b = np.zeros((hidden_unit_num, 1))\n",
        "    for i in range(5):\n",
        "        # First polygon\n",
        "        x1 = p0[i, 0]\n",
        "        y1 = p0[i, 1]\n",
        "        x2 = p0[(i+1)%5, 0]\n",
        "        y2 = p0[(i+1)%5, 1]\n",
        "        W[i, :] = [y1 - y2, x2 - x1]\n",
        "        b[i, :] = x1 * y2 - x2 * y1\n",
        "        # Second polygon\n",
        "        x1 = p1[i, 0]\n",
        "        y1 = p1[i, 1]\n",
        "        x2 = p1[(i+1)%5, 0]\n",
        "        y2 = p1[(i+1)%5, 1]\n",
        "        W[i + 5, :] = [y1 - y2, x2 - x1]\n",
        "        b[i + 5, :] = x1 * y2 - x2 * y1\n",
        "    return W,b\n",
        "\n",
        "def predict_output_v1(X, W, b):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for idx in range(data.shape[0]):\n",
        "        x = np.reshape(X[idx, :], (2, 1))\n",
        "        # First layer\n",
        "        first_layer_output = np.matmul(W, x) + b\n",
        "        first_layer_output = threshold_activation1(first_layer_output)\n",
        "        # Second layer\n",
        "        first_polygon = first_layer_output[0:5, :]\n",
        "        second_polygon = first_layer_output[5:10, :]\n",
        "        first_gate_output = and_gate(first_polygon)\n",
        "        second_gate_output = and_gate(second_polygon)\n",
        "        # Output layer\n",
        "        input_to_final_gate = [first_gate_output, second_gate_output]\n",
        "        prediction = or_gate(input_to_final_gate)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "def predict_output_v2(X, W, b):\n",
        "    \"\"\"\n",
        "    #TODO: Update usage of the gates in this function\n",
        "    \"\"\"\n",
        "    ## Cache of Predictions\n",
        "    predictions = []\n",
        "    ## Cycle Through Data Points\n",
        "    for idx in range(data.shape[0]):\n",
        "        x = np.reshape(X[idx, :], (2, 1))\n",
        "        # First layer\n",
        "        first_layer_output = np.matmul(W, x) + b\n",
        "        first_layer_output = threshold_activation1(first_layer_output)\n",
        "        # Second layer\n",
        "        first_polygon = first_layer_output[0:5, :]\n",
        "        second_polygon = first_layer_output[5:10, :]\n",
        "        first_gate_output = and_gate(first_polygon)\n",
        "        # second_gate_output = and_gate(second_polygon)\n",
        "        # Output layer\n",
        "       # input_to_final_gate = [first_gate_output, second_gate_output]\n",
        "        prediction = first_gate_output\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "def calc_accuracy(true_y, pred_y):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    true_prediction_num = 0\n",
        "    for i, py in enumerate(pred_y):\n",
        "        if py == true_y[i]:\n",
        "            true_prediction_num += 1\n",
        "    accuracy = true_prediction_num / len(pred_y)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSnNFYhu0gDC"
      },
      "source": [
        "*Sanity check:* If you correctly implemented the 'and gate' and 'or gate', all points should be classified correctly when you make predictions using `predict_output_v1()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pknSuuIXrG7Y"
      },
      "outputs": [],
      "source": [
        "## Load Our Parameters\n",
        "W, b = analytical_parameters(p0, p1)\n",
        "\n",
        "## Make Predictions\n",
        "pred_Y = predict_output_v1(X, W, b)\n",
        "\n",
        "## Compute Accuracy\n",
        "acc = calc_accuracy(Y, pred_Y)\n",
        "assert (acc == 1)\n",
        "\n",
        "## Visualize Predictions\n",
        "visualize_datapoints(X, np.array(pred_Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME0TSvkX2KWg"
      },
      "source": [
        "In the code above, change the gates in `predict_output_v2()` such that only the points in the top polygon are classified correctly. Visualize your result, report the accuracy of this model, and attach it to the submission.\n",
        "\n",
        "To further clarify, you should **only** change the usage of the gating functions, not the code inside the gating function itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szTyPVIp1nXq"
      },
      "outputs": [],
      "source": [
        "## Load Our Parameters\n",
        "W, b = analytical_parameters(p0, p1)\n",
        "\n",
        "## Make Predictions\n",
        "pred_Y = predict_output_v2(X, W, b)\n",
        "\n",
        "## Visualize Predictions\n",
        "visualize_datapoints(X, np.array(pred_Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "216MojZrPDFJ"
      },
      "source": [
        "## Problem 1b-d)\n",
        "\n",
        "Complete problems 1b through 1d in the space below. Please use markdown to clearly distinguish your answers for each part. Include appropriate visualizations generated here in your final report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOMhe2lu7mC5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# problem 1.b\n",
        "\n",
        "\n",
        "class MLP1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.first_layer = nn.Linear(2, 10)\n",
        "    self.second_layer = nn.Linear(10, 2)\n",
        "    self.output = nn.Linear(2, 1)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "    for layer in [self.first_layer, self.second_layer, self.output]:\n",
        "      nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.activation(self.first_layer(x))\n",
        "    x = self.activation(self.second_layer(x))\n",
        "    x = self.activation(self.output(x))\n",
        "\n",
        "    return x\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(y_hat, y):\n",
        "  prediction = (y_hat > 0.5).float()\n",
        "  return (prediction == y).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train(model, x, y, test_x, test_y, loss_function, accuracy_function, batch_size = 128, max_epochs = 500, lr = 0.01):\n",
        "  loss_function = nn.BCELoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "  if isinstance(x, np.ndarray):\n",
        "    x = torch.FloatTensor(x)\n",
        "\n",
        "  if isinstance(y, np.ndarray):\n",
        "    y = torch.FloatTensor(y)\n",
        "\n",
        "  if isinstance(test_x, np.ndarray):\n",
        "    test_x = torch.FloatTensor(test_x)\n",
        "\n",
        "  if isinstance(test_y, np.ndarray):\n",
        "    test_y = torch.FloatTensor(test_y)\n",
        "\n",
        "  if batch_size is None:\n",
        "    batch_size = len(x)\n",
        "\n",
        "  dataset = torch.utils.data.TensorDataset(x, y)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  test_accuracy = []\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for batch_x, batch_y in dataloader:\n",
        "\n",
        "      y_hat = model(batch_x)\n",
        "      loss = loss_function(y_hat, batch_y)\n",
        "      acc = accuracy_function(y_hat, batch_y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc\n",
        "\n",
        "    accuracies.append(epoch_acc / len(dataloader))\n",
        "    losses.append(epoch_loss / len(dataloader))\n",
        "\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    avg_acc = epoch_acc / len(dataloader)\n",
        "\n",
        "    # evaluation\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      test_prediction = (model(test_x) > 0.5).float()\n",
        "      test_acc = (test_prediction.eq(test_y).sum().item()) / len(test_y)\n",
        "      test_accuracy.append(test_acc)\n",
        "\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == max_epochs - 1:\n",
        "\n",
        "      print(\"iteration\",epoch,\"loss:\", avg_loss, \"accuracy:\",avg_acc)\n",
        "\n",
        "\n",
        "\n",
        "  fig0=plt.figure(0)\n",
        "  plt.plot(losses,'-')\n",
        "  plt.xlabel('Iteration', fontsize=18)\n",
        "  plt.ylabel('Loss', fontsize=16)\n",
        "  plt.show()\n",
        "  fig1=plt.figure(1)\n",
        "  plt.plot(accuracies,'-')\n",
        "  plt.xlabel('Iteration', fontsize=18)\n",
        "  plt.ylabel('Accuracy', fontsize=16)\n",
        "  plt.show()\n",
        "\n",
        "  return losses, accuracies, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean = X.mean(axis = 0)\n",
        "std = X.std(axis = 0)\n",
        "x_norm = (X - mean) / std\n",
        "\n",
        "\n",
        "x_train, x_test = x_norm[:50000], x_norm[50000:]\n",
        "y_train, y_test = Y[:50000], Y[50000:]\n",
        "\n",
        "\n",
        "X_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "seeds = [0, 1, 2, 3, 4]\n",
        "results = []\n",
        "\n",
        "\n",
        "train_accuracy, test_accuracy = [], []\n",
        "\n",
        "for seed in seeds:\n",
        "  torch.manual_seed(seed)\n",
        "  model = MLP1()\n",
        "  \n",
        "  losses, train_accs, test_accs = train(\n",
        "        model,\n",
        "        X_train, Y_train,\n",
        "        X_test, Y_test,\n",
        "        nn.BCELoss(),\n",
        "        accuracy,\n",
        "        batch_size=128,\n",
        "        max_epochs=500,\n",
        "        lr=0.01\n",
        "    )\n",
        "  results.append((losses, train_accs, test_accs))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "for i, (losses, train_accs, test_accs) in enumerate(results):\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(losses, label=f'Seed {i}')\n",
        "  plt.title('Training Loss')\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(test_accs, label=f'Seed {i}')\n",
        "  plt.title('Test Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = (model(torch.FloatTensor(x_test)) > 0.5).numpy().astype(int).flatten()\n",
        "\n",
        "visualize_datapoints(x_test * std + mean, preds)  # rescale back to original coords\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "final_train_accs = [r[1][-1] for r in results]\n",
        "final_test_accs = [r[2][-1] for r in results]\n",
        "\n",
        "print(f\"Mean Train Acc: {np.mean(final_train_accs):.4f} ± {np.std(final_train_accs):.4f}\")\n",
        "print(f\"Mean Test Acc:  {np.mean(final_test_accs):.4f} ± {np.std(final_test_accs):.4f}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MLP2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "            nn.Linear(2, 32),  \n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(8, 4),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(4, 1),\n",
        "            nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    for layer in self.layers:\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean = X.mean(axis = 0)\n",
        "std = X.std(axis = 0)\n",
        "x_norm = (X - mean) / std\n",
        "\n",
        "\n",
        "x_train, x_test = x_norm[:50000], x_norm[50000:]\n",
        "y_train, y_test = Y[:50000], Y[50000:]\n",
        "\n",
        "\n",
        "X_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "seeds = [0, 1, 2, 3, 4]\n",
        "results = []\n",
        "\n",
        "\n",
        "train_accuracy, test_accuracy = [], []\n",
        "\n",
        "for seed in seeds:\n",
        "  torch.manual_seed(seed)\n",
        "  model = MLP2()\n",
        "  \n",
        "  losses, train_accs, test_accs = train(\n",
        "        model,\n",
        "        X_train, Y_train,\n",
        "        X_test, Y_test,\n",
        "        nn.BCELoss(),\n",
        "        accuracy,\n",
        "        batch_size=128,\n",
        "        max_epochs=500,\n",
        "        lr=0.01\n",
        "    )\n",
        "  results.append((losses, train_accs, test_accs))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "for i, (losses, train_accs, test_accs) in enumerate(results):\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(losses, label=f'Seed {i}')\n",
        "  plt.title('Training Loss')\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(test_accs, label=f'Seed {i}')\n",
        "  plt.title('Test Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = (model(torch.FloatTensor(x_test)) > 0.5).numpy().astype(int).flatten()\n",
        "\n",
        "visualize_datapoints(x_test * std + mean, preds)  # rescale back to original coords\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "final_train_accs = [r[1][-1] for r in results]\n",
        "final_test_accs = [r[2][-1] for r in results]\n",
        "\n",
        "print(f\"Mean Train Acc: {np.mean(final_train_accs):.4f} ± {np.std(final_train_accs):.4f}\")\n",
        "print(f\"Mean Test Acc:  {np.mean(final_test_accs):.4f} ± {np.std(final_test_accs):.4f}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sxSgG9wPDFK"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "All code for Problem 2 should go below. We provide data loaders and relevant imports to get you started. If you are working locally (instead of using Google Colab), we recommend using Conda to install pytorch (https://pytorch.org).\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcocBj9hPDFK"
      },
      "outputs": [],
      "source": [
        "## Additional External Libraries (Deep Learning)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as tfs\n",
        "from PIL import Image\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKYtXumUPDFK"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGEgukvLPDFK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter (Feel Free to Change These, but Make Sure your Training Loop Still Works as Expected)\n",
        "TRAIN_BATCH_SIZE = 50\n",
        "VAL_BATCH_SIZE = 50\n",
        "TEST_BATCH_SIZE = 1\n",
        "\n",
        "# Transform data to PIL images\n",
        "transforms = tfs.Compose([tfs.ToTensor()])\n",
        "\n",
        "# Train/Val Subsets\n",
        "train_mask = range(50000)\n",
        "val_mask = range(50000, 60000)\n",
        "\n",
        "# Download/Load Dataset\n",
        "train_dataset = FashionMNIST('./data', train=True, transform=transforms, download=True)\n",
        "test_dataset = FashionMNIST('./data', train=False, transform=transforms, download=True)\n",
        "\n",
        "# Data Loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, sampler=SubsetRandomSampler(train_mask))\n",
        "val_dataloader = DataLoader(train_dataset, batch_size=VAL_BATCH_SIZE, sampler=SubsetRandomSampler(val_mask))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqfCSiCdPDFK"
      },
      "source": [
        "## Problem 2a)\n",
        "\n",
        "### Design Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWczeepjPDFK"
      },
      "outputs": [],
      "source": [
        "class CNNet(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 max pooling\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(16*7*7, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))  \n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))  \n",
        "        x = torch.flatten(x, 1)               \n",
        "        x = nn.functional.relu(self.fc1(x))              \n",
        "        x = self.fc2(x)                        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvczCx2ZPDFK"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgepUwPrPDFK"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          optimizer,\n",
        "          loss,\n",
        "          lr,\n",
        "          epochs=50,\n",
        "          train_dataloader=train_dataloader,\n",
        "          val_dataloader=val_dataloader,\n",
        "          test_dataloader=test_dataloader,\n",
        "          **kwargs):\n",
        "    \"\"\"\n",
        "    #TODO: Implement a training loop\n",
        "    # Your function can return the model, train loss, and validation accuracy\n",
        "    \"\"\"\n",
        "        \n",
        "\t\tfor epoch in range(epochs):\n",
        "\t\t\tmodel.train()\n",
        "\t\t\tloss = 0.0\n",
        "\t\t\tcorrect = 0\n",
        "\t\t\ttotal = 0\n",
        "\n",
        "\n",
        "\t\t\tfor inputs, labels in train_dataloader:\n",
        "\t\t\t\tinputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\toutputs = model(inputs)\n",
        "\t\t\t\tloss = loss(outputs, labels)\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\tloss += loss.item()\n",
        "\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\ttotal += labels.size(0)\n",
        "\t\t\t\tcorrect += (predicted == labels).sum().item()\t\n",
        "\n",
        "\t\ttrain_accuracy = correct / total\n",
        "\n",
        "\n",
        "\t\tmodel.eval()\n",
        "\t\tval_correct = 0\n",
        "\t\tval_total = 0\n",
        "\t\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor inputs, labels in val_dataloader:\n",
        "\t\t\t\tinputs, labels = inputs.cuda(), labels.cuda()\n",
        "\t\t\t\toutputs = model(inputs)\n",
        "\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\tval_total += labels.size(0)\n",
        "\t\t\t\tval_correct += (predicted == labels).sum().item()\n",
        "\t\tval_accuracy = val_correct / val_total\n",
        "\t\n",
        "  \n",
        "    return model\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmV08pS6PDFK"
      },
      "outputs": [],
      "source": [
        "## Hyperparameters\n",
        "EPOCH = 50\n",
        "LR = ##TODO: Choose a Learning Rate\n",
        "\n",
        "## Setting up the model, optimizer, and loss function\n",
        "model = CNNet()\n",
        "optimizer = ##TODO: Choose an optimized\n",
        "loss_f = ##TODO: Choose a loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnmWFEt3PDFK"
      },
      "outputs": [],
      "source": [
        "## Run Training Loop\n",
        "out = train(model, optimizer, loss_f, LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPN1uK8KPDFL"
      },
      "source": [
        "## Problem 2b)\n",
        "\n",
        "Now try to improve your model using additional techniques learned during class. You should be able to use the same training function as above, but will need to create a new model architecture.\n",
        "\n",
        "### Data Loading\n",
        "\n",
        "You should maintain the splits from above, but feel free to alter the dataloaders (i.e. transforms) as you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar110CBLPDFL"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter (Feel Free to Change These, but Make Sure your Training Loop Still Works as Expected)\n",
        "TRAIN_BATCH_SIZE = 50\n",
        "VAL_BATCH_SIZE = 50\n",
        "TEST_BATCH_SIZE = 1\n",
        "\n",
        "# Transform data to PIL images\n",
        "transforms = ##TODO: Use the same from above or consider alternatives\n",
        "\n",
        "# Train/Val Subsets\n",
        "train_mask = range(50000)\n",
        "val_mask = range(50000, 60000)\n",
        "\n",
        "# Download/Load Dataset\n",
        "train_dataset = FashionMNIST('./data', train=True, transform=transforms, download=True)\n",
        "test_dataset = FashionMNIST('./data', train=False, transform=transforms, download=True)\n",
        "\n",
        "# Data Loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, sampler=SubsetRandomSampler(train_mask))\n",
        "val_dataloader = DataLoader(train_dataset, batch_size=VAL_BATCH_SIZE, sampler=SubsetRandomSampler(val_mask))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8F7EIxXPDFL"
      },
      "source": [
        "### Model Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYa6pKlmPDFL"
      },
      "outputs": [],
      "source": [
        "class CNNImproved(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNImproved, self).__init__()\n",
        "\t\t\t\tself.conv_layers = nn.Sequential(\t\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "\t\t\t\tself.fc_layers = nn.Sequential(\n",
        "\t\t\t\t\t\tnn.Dropout(0.5),\n",
        "            nn.Linear(128*3*3, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "\t\t\t\t)\n",
        "        \n",
        "\t\tdef forward(self, x):\n",
        "\t\t\t\tx = self.conv_layers(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-vJ08i8PDFL"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSzH1mTLPDFL"
      },
      "outputs": [],
      "source": [
        "model = CNNImproved()\n",
        "optimizer =torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaGzm3x7PDFL"
      },
      "source": [
        "## Problem 2c)\n",
        "\n",
        "Write down your response in the final report."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
