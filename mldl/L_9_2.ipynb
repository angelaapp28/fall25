{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bNXCy0voYj8"
   },
   "source": [
    "##2. Image Segmentation with DICE Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5gpUtkZYX5R"
   },
   "source": [
    "* Like in L-9-1, you are given a training loop for MNIST.\n",
    "* However, in this case you will be performing image segmentation on the dataset with added noise.\n",
    "* You will be introduced to the DICE loss and will need to implement such that it works with pytorch and the training loop.\n",
    "* **Deliverables:**\n",
    "    * Implement DICE loss such that the network trains with no errors thrown.\n",
    "    * Describe what part of the formulation needed to be adjusted and for what reason.\n",
    "    * Achieve a specific performance benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2xHDu65a3ji"
   },
   "source": [
    "### 2.1 What is image segmentation?\n",
    "\n",
    "* Image segmentation tasks consist of categorizing different parts of an image.\n",
    "* Typically this is done pixel-wise.\n",
    "* Pixel-wise segmentation is equilavent to classifying each pixel.\n",
    "* Below is an example of image segmentation: (Taken from [this blog](https://nanonets.com/blog/how-to-do-semantic-segmentation-using-deep-learning/).)\n",
    "![](https://cdn-images-1.medium.com/max/900/1*MQCvfEbbA44fiZk5GoDvhA.png)\n",
    "* Each color represents a different category of the image.\n",
    "* In this case, the neural network classified each pixel as one category (smoothing can also be applied afterwards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGHSuntbbu5P"
   },
   "source": [
    "### 2.2 What is Dice Loss?\n",
    "\n",
    "* One popular loss function for image segmentation is the Dice loss.\n",
    "* This originates from the Sørensen–Dice coefficient, a metric of similarity between two sets.\n",
    "* For discrete sets $A$ and $B$, the Dice coefficient is defined as:\n",
    "$$DSC = \\frac{2| A \\cap B | }{|A| + |B|}$$\n",
    "* In other words, it is the proportion of common elements in $A$ and $B$. For DSC of 0, there is no overlap, for DSC of 1, $A = B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvL4wuLBeGGW"
   },
   "source": [
    "### 2.3 Applying Dice Loss to Machine Learning\n",
    "* How can the Dice coefficient be adapted for a Machine Learning task?\n",
    "* For binary segmentation (segmenting the image into two regions), for an image with $d$ pixels, the network output $f(x) \\in [0,1]^d$, where $f(x)_k$ corresponds to the probability that the $k$-th pixel belongs to class 1, and $1- f(x)_k$ corresponds to the probability that the $k$-th pixel belongs to class 2.\n",
    "* For implementation purposes, consider ground-truth $y = [0,0,0,1,1,1,0,0]$ and prediction $\\hat{y} = [0.03,0.12,0.01,0.98,0.97,0.99,0.01,0.2]$.\n",
    "* First, let's translate $| y \\cap \\hat{y}|$ into an arithmetic operation (which would work with auto-differentiation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGYE_zvWazbw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.array([0,0,0,1,1,1,0,0])\n",
    "y_hat = np.array([0.03,0.12,0.01,0.98,0.97,0.99,0.01,0.2])\n",
    "\n",
    "## Define similarity method (numerator only of DSC) here:\n",
    "def DSC_num(A,B):\n",
    "  ## [Your Code here]\n",
    "  return\n",
    "\n",
    "print(DSC_num(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0YldR80QiOF_"
   },
   "source": [
    "* Does your `DSC_num` function capture perfect overlap (i.e. returns 1 when A = B) and perfect non-overlap (i.e. reutrns 0 when $|A \\cap B| = 0$)?\n",
    "* Next, let's translate $|A|$ into an arithmetic operation as well and then define our DSC loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0WV_7BSjDz9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mDSC\u001b[39m(A,B):  \n\u001b[32m      9\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m DSC_num(A,B) / DSC_den(A,B)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mDSC\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mDSC\u001b[39m\u001b[34m(A, B)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mDSC\u001b[39m(A,B):  \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDSC_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mDSC_den\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for /: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "## Define sum (denominator only of DSC) method here:\n",
    "def DSC_den(A,B):\n",
    "  ## [Your Code here]\n",
    "  return\n",
    "\n",
    "print(DSC_den(y, y_hat))\n",
    "\n",
    "def DSC(A,B):  \n",
    "  return DSC_num(A,B) / DSC_den(A,B)\n",
    "\n",
    "print(DSC(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFBNO_iTjeLU"
   },
   "source": [
    "* If you haven't already, define these functions such that they work with pytorch tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymzTY6pMj0lR"
   },
   "outputs": [],
   "source": [
    "y = torch.from_numpy(np.array([0,0,0,1,1,1,0,0])).float()\n",
    "y_hat = torch.from_numpy(np.array([0.03,0.12,0.01,0.98,0.97,0.99,0.01,0.2])).float()\n",
    "\n",
    "print(DSC_num(y, y_hat))\n",
    "print(DSC_den(y, y_hat))\n",
    "print(DSC(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z03QOIQOwynl"
   },
   "source": [
    "*   You will also need to define these functions such that they work with minibatches of shape (batch_size, 784). Below, we give an example of a batch size of 4.\n",
    "*   Your `DSC_batch` function will need to return a scalar. To do this, return the mean of the DSC loss for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRlKoMnQxA3B"
   },
   "outputs": [],
   "source": [
    "A = [0,0,0,1,1,1,0,0]\n",
    "B = [0.03,0.12,0.01,0.98,0.97,0.99,0.01,0.2]\n",
    "y = torch.from_numpy(np.array([A,A,A,A])).float()\n",
    "y_hat = torch.from_numpy(np.array([B,B,B,B])).float()\n",
    "print(\"Data shapes:\",y.shape, y_hat.shape)\n",
    "\n",
    "def DSC_num_batch(A,B):\n",
    "  ## [Your Code here]\n",
    "  return\n",
    "\n",
    "def DSC_den_batch(A,B):\n",
    "  ## [Your Code here]\n",
    "  return\n",
    "\n",
    "def DSC_batch(A,B):\n",
    "  return (DSC_num_batch(A,B) / DSC_den_batch(A,B)).mean()\n",
    "\n",
    "def DSC_loss(A,B):\n",
    "  return 1 + -1.0*DSC_batch(A,B)\n",
    "\n",
    "print(\"DSC function outputs:\")\n",
    "print(DSC_num_batch(y, y_hat))\n",
    "print(DSC_den_batch(y, y_hat))\n",
    "print(DSC_batch(y, y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jT0xdt6ykPnk"
   },
   "source": [
    "### 2.4 Using Dice With Noisy MNIST Segmentation\n",
    "\n",
    "Now, we are ready to test our Dice loss on binary segmentation of MNIST with noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1bBnjipktdL"
   },
   "source": [
    "**Defining the model and optimizer:**\n",
    "We define the model and optimizer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tx0znMC0ksVi"
   },
   "outputs": [],
   "source": [
    "## Defining the model:\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, input_size, width, num_classes):\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    ##feedfoward layers:\n",
    "    self.ff1 = nn.Linear(input_size, width) #input\n",
    "\n",
    "    self.ff2 = nn.Linear(width, width) #hidden layers\n",
    "    self.ff3 = nn.Linear(width, width)\n",
    "\n",
    "    self.ff_out = nn.Linear(width, num_classes) #logit layer     \n",
    "\n",
    "    ##activations:\n",
    "    self.relu = nn.ReLU()\n",
    "    self.sm = nn.Softmax()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "                \n",
    "  def forward(self, input_data):\n",
    "    out = self.relu(self.ff1(input_data)) \n",
    "    out = self.relu(self.ff2(out)) \n",
    "    out = self.relu(self.ff3(out))\n",
    "    out = self.ff_out(out)\n",
    "    return self.sigmoid(out) #returns probabilities for each pixel\n",
    "\n",
    "net = Net(input_size = 784, width = 500, num_classes = 784)\n",
    "if gpu_boole:\n",
    "  net = net.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.05)\n",
    "loss_metric = DSC_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96OjGKdllLzh"
   },
   "source": [
    "**Data loading:** In L-9-1, we made use of the `torchvision` package for data-loading and preprocessing. We abstracted away some preprocessing steps. Here, we are giving a more granular implementation that you may have to adjust in various ways. This is more akin to what you will see in practice with other datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqOBK1oRoiYM"
   },
   "outputs": [],
   "source": [
    "#Downloading and unzipping MNIST data files:\n",
    "!curl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "!curl -O http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "!curl -O http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "!curl -O http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "!gunzip t*-ubyte.gz -f\n",
    "\n",
    "##Loading files into numpy arrays and then torch Tensorsz:\n",
    "def read_idx(filename, boole=0):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        if boole:\n",
    "          return np.fromstring(f.read(), dtype=np.uint8).reshape(shape).astype(np.float32)*10     \n",
    "        else:\n",
    "          return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "xtrain = read_idx('train-images-idx3-ubyte', 1)\n",
    "xtest = read_idx('t10k-images-idx3-ubyte', 1)\n",
    "\n",
    "xtrain = torch.Tensor(xtrain)\n",
    "xtest = torch.Tensor(xtest)\n",
    "\n",
    "##normalizing between 0 and 1:\n",
    "xtrain -= xtrain.min()\n",
    "xtest -= xtest.min()\n",
    "xtrain /= xtrain.max()\n",
    "xtest /= xtest.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nb_RMiSMoqoV"
   },
   "source": [
    "Of note, here, is that we use the original MNIST dataset for our ground-truth values. Given the simplicity of MNIST, simple rounding gives us close to the ground-truth for binary segmentation. In practice, with more complex datasets, there are more detailed images where simple rounding does not work, and manually labeled ground truth values must be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d57usRkyorF2"
   },
   "outputs": [],
   "source": [
    "ytrain = torch.round(xtrain)\n",
    "ytest = torch.round(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nB7v92rElPv1"
   },
   "source": [
    "**Adding noise:** Here, we add Gaussian noise with mean of 0.01 and standard deviation of 0.1 to a single MNIST image for illustration. This is to make the task harder and not too simplistic for this assignment. We will incorporate this noise adding into the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DxTil3SmD4c"
   },
   "outputs": [],
   "source": [
    "# Image with noise:\n",
    "print(\"Image with noise:\")\n",
    "plt.imshow((xtrain[0] + xtrain[0].data.new(xtrain[0].size()).normal_(0.01, 0.1)).cpu().data.numpy(), cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Ground truth:\n",
    "print(\"Image segmentation ground-truth:\")\n",
    "plt.imshow(ytrain[0].cpu().data.numpy(), cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "# adding noise to all images:\n",
    "for i in range(xtrain.shape[0]):\n",
    "  xtrain[i] += xtrain[i].data.new(xtrain[i].size()).normal_(0.01, 0.1)\n",
    "for i in range(xtest.shape[0]):\n",
    "  xtest[i] += xtest[i].data.new(xtest[i].size()).normal_(0.01, 0.1)\n",
    "\n",
    "# dataloaders:\n",
    "train = torch.utils.data.TensorDataset(xtrain, ytrain)\n",
    "test = torch.utils.data.TensorDataset(xtest, ytest)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GffUQPvLsOci"
   },
   "source": [
    "**Defining training and test loss functions:** These functions will be useful in our training loop to view are training and test loss at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JCypKzPsOxI"
   },
   "outputs": [],
   "source": [
    "def train_eval(verbose = 1):\n",
    "    loss_sum = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        if gpu_boole:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels.view(-1, 28*28)\n",
    "        outputs = net(images)\n",
    "        loss_sum += loss_metric(outputs,labels)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    if verbose:\n",
    "        print('Train loss: %f' % (loss_sum.cpu().data.numpy().item() / total))\n",
    "\n",
    "    return loss_sum.cpu().data.numpy().item() / total\n",
    "    \n",
    "def test_eval(verbose = 1):\n",
    "    loss_sum = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        if gpu_boole:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels.view(-1, 28*28)\n",
    "        outputs = net(images)\n",
    "        loss_sum += loss_metric(outputs,labels)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    if verbose:\n",
    "        print('Test loss: %f' % (loss_sum.cpu().data.numpy().item() / total))\n",
    "\n",
    "    return loss_sum.cpu().data.numpy().item() / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "naOqG870mEe8"
   },
   "source": [
    "**Training loop:** We are finally ready to begin training. Your own `DSC_pytorch` is called for computing the loss. It's possible you may need to debug this in the loop. We given an output of your segmented image and compare it to ground truth below.\n",
    "\n",
    "**IMPORTANT NOTE:** For re-running this code cell, if you encounter nan loss, you will need to reinstantiate your model and optimizer by re-running the \"Defining the model and optimizer:\" code cell above.\n",
    "\n",
    "If you modify your DSC_loss definitions, you will also need to re-run the \"Defining the model and optimizer:\" code cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2xuEzVhmiez"
   },
   "outputs": [],
   "source": [
    "#re-initializing network weights:\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "\n",
    "weights_init(net)\n",
    "\n",
    "#number of epochs to train for:\n",
    "epochs = 10\n",
    "\n",
    "#defining batch train loss recording arrays for later visualization/plotting:\n",
    "loss_batch_store = []\n",
    "\n",
    "print(\"Starting Training\")\n",
    "#training loop:\n",
    "for epoch in range(epochs):\n",
    "  time1 = time.time() #timekeeping\n",
    "\n",
    "  for i, (x,y) in enumerate(train_loader):\n",
    "\n",
    "    if gpu_boole:\n",
    "      x = x.cuda()\n",
    "      y = y.cuda()\n",
    "\n",
    "    x = x.view(x.shape[0],-1)\n",
    "    y = y.view(y.shape[0],-1)\n",
    "\n",
    "    #loss calculation and gradient update:\n",
    "\n",
    "    if i > 0 or epoch > 0:\n",
    "      optimizer.zero_grad()\n",
    "    outputs = net.forward(x)\n",
    "    loss = loss_metric(outputs,y)\n",
    "    loss.backward()\n",
    "\n",
    "    if i > 0 or epoch > 0:\n",
    "      loss_batch_store.append(loss.cpu().data.numpy().item())\n",
    "                  \n",
    "    ##performing update:\n",
    "    optimizer.step()\n",
    "\n",
    "  print(\"Epoch\",epoch+1,':')\n",
    "  train_loss = train_eval()\n",
    "  test_loss = test_eval()\n",
    "\n",
    "  time2 = time.time() #timekeeping\n",
    "  print('Elapsed time for epoch:',time2 - time1,'s')\n",
    "  print('ETA of completion:',(time2 - time1)*(epochs - epoch - 1)/60,'minutes')\n",
    "  print()\n",
    "\n",
    "## Plotting batch-wise train loss curve:\n",
    "plt.plot(loss_batch_store, '-o', label = 'train_loss', color = 'blue')\n",
    "plt.xlabel('Minibatch Number')\n",
    "plt.ylabel('Sample-wise Loss At Last minibatch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## Plotting sample xtest segmentation and prediction:\n",
    "print(\"Noisy xtest image:\")\n",
    "plt.imshow(xtest[0].cpu().data.numpy(), cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Ground-truth:\")\n",
    "plt.imshow(ytest[0].cpu().data.numpy(), cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction:\")\n",
    "if gpu_boole:\n",
    "  out = net.forward(xtest[0].cuda().view(1,28*28)).view(28,28).cpu().data.numpy()\n",
    "else:\n",
    "  out = net.forward(xtest[0].view(1,28*28)).view(28,28).cpu().data.numpy()\n",
    "plt.imshow(out, cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVkME6-D5z4s"
   },
   "source": [
    "**Unit testing:**\n",
    "\n",
    "Okay, does your final output look correct? Even if you network is segmenting correctly, it may not be doing so optimally. You should unit test your DSC_batch function to make sure it is returning the correct value.\n",
    "\n",
    "Hand-calculate the DSC loss for two specific inputs, and make sure your DSC_batch function is returning the correct value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1A4nqme_6ft4"
   },
   "outputs": [],
   "source": [
    "#Unit testing DSC_batch:\n",
    "\n",
    "#TO-DO: Insert your specific testing vectors here:\n",
    "y1 = []\n",
    "y1_hat = []\n",
    "\n",
    "#O-DO: Insert your specific testing vectors here:\n",
    "y2 = []\n",
    "y2_hat = []\n",
    "\n",
    "y = torch.from_numpy(np.array([y1,y2])).float()\n",
    "y_hat = torch.from_numpy(np.array([y1_hat,y2_hat])).float()\n",
    "print(\"Data shapes:\",y.shape, y_hat.shape)\n",
    "\n",
    "print(\"DSC function outputs:\")\n",
    "print(DSC_num_batch(y, y_hat))\n",
    "print(DSC_den_batch(y, y_hat))\n",
    "print(DSC_batch(y, y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vptvhkarmlEc"
   },
   "source": [
    "**Questions:**\n",
    "* How did you implement the overlap metric and the sum metric? What adjustments were needed (if any) to made to make it amenable to pytorch?\n",
    "\n",
    "[Your text here.]\n",
    "\n",
    "* Why might the Dice loss be advantageous compared to cross entropy? \n",
    "\n",
    "[Your text here.]\n",
    "\n",
    "* Are there cases where cross entropy might be advantageous compared to the Dice loss? \n",
    "\n",
    "[Your text here.]\n",
    "\n",
    "* Did you unit test your Dice loss? Did it help you catch errors or what it already correctly specified? \n",
    "\n",
    "[Your text here.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ix0mGS9n25Tb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgZHdwzKnb8u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "L_9_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
